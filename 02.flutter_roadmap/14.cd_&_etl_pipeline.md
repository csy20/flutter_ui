# CI/CD & ETL Pipelines

CI/CD and ETL pipelines are automation tools that help streamline development, deployment, and data processing workflows.

## üìö What is CI/CD?

**CI/CD** stands for **Continuous Integration** and **Continuous Deployment/Delivery**

### **Continuous Integration (CI)**
- Automatically tests your code when you push changes
- Catches bugs early before they reach production
- Ensures code quality and consistency

### **Continuous Deployment (CD)**
- Automatically deploys your app to production
- Reduces manual deployment errors
- Faster release cycles

---

## üîÑ How CI/CD Works (Simple Flow)

```
Developer pushes code ‚Üí CI runs tests ‚Üí If tests pass ‚Üí CD deploys app
                                   ‚Üì
                               If tests fail ‚Üí Notify developer
```

### Real-World Example:
1. You fix a bug in your Flutter app
2. You push code to GitHub
3. CI automatically runs tests
4. If tests pass, CD deploys to app stores
5. Users get the update automatically

---

## üöÄ CI/CD for Flutter Apps

### 1. GitHub Actions Example

**`.github/workflows/flutter_ci_cd.yml`**
```yaml
name: Flutter CI/CD Pipeline

# When to run this pipeline
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  # Continuous Integration Job
  test:
    runs-on: ubuntu-latest
    
    steps:
    # Get the code
    - name: Checkout code
      uses: actions/checkout@v3
    
    # Setup Flutter
    - name: Setup Flutter
      uses: subosito/flutter-action@v2
      with:
        flutter-version: '3.16.0'
    
    # Get dependencies
    - name: Install dependencies
      run: flutter pub get
    
    # Run code analysis
    - name: Analyze code
      run: flutter analyze
    
    # Run tests
    - name: Run tests
      run: flutter test
    
    # Build app (optional)
    - name: Build APK
      run: flutter build apk --release

  # Continuous Deployment Job
  deploy:
    needs: test  # Only run if tests pass
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'  # Only deploy from main branch
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Setup Flutter
      uses: subosito/flutter-action@v2
      with:
        flutter-version: '3.16.0'
    
    - name: Build release APK
      run: flutter build apk --release
    
    # Deploy to Play Store (example)
    - name: Deploy to Play Store
      uses: r0adkll/upload-google-play@v1
      with:
        serviceAccountJsonPlainText: ${{ secrets.GOOGLE_PLAY_SERVICE_ACCOUNT }}
        packageName: com.example.myapp
        releaseFiles: build/app/outputs/flutter-apk/app-release.apk
        track: production
```

### 2. Simple Deployment Script

**`scripts/deploy.sh`**
```bash
#!/bin/bash

echo "üöÄ Starting deployment process..."

# Build the app
echo "üì± Building Flutter app..."
flutter clean
flutter pub get
flutter build apk --release

# Run tests
echo "üß™ Running tests..."
flutter test

# Check if tests passed
if [ $? -eq 0 ]; then
    echo "‚úÖ Tests passed! Deploying..."
    
    # Copy APK to deployment folder
    cp build/app/outputs/flutter-apk/app-release.apk ./releases/
    
    # Upload to server (example)
    # scp app-release.apk user@server:/var/www/downloads/
    
    echo "üéâ Deployment successful!"
else
    echo "‚ùå Tests failed! Deployment cancelled."
    exit 1
fi
```

### 3. Firebase App Distribution

**`firebase_deploy.yml`**
```yaml
name: Deploy to Firebase App Distribution

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Flutter
      uses: subosito/flutter-action@v2
      with:
        flutter-version: '3.16.0'
    
    - name: Build APK
      run: |
        flutter pub get
        flutter build apk --release
    
    - name: Deploy to Firebase App Distribution
      uses: wzieba/Firebase-Distribution-Github-Action@v1
      with:
        appId: ${{ secrets.FIREBASE_APP_ID }}
        token: ${{ secrets.FIREBASE_TOKEN }}
        groups: testers
        file: build/app/outputs/flutter-apk/app-release.apk
```

---

## üìä What is ETL Pipeline?

**ETL** stands for **Extract, Transform, Load**

### **Extract**
- Get data from various sources
- Databases, APIs, files, etc.

### **Transform**
- Clean and process the data
- Format, validate, calculate

### **Load**
- Save processed data to destination
- Database, data warehouse, files

---

## üîß How ETL Works (Simple Flow)

```
Data Sources ‚Üí Extract ‚Üí Transform ‚Üí Load ‚Üí Data Warehouse/Database
    ‚Üì              ‚Üì         ‚Üì         ‚Üì
Files, APIs,   Read data   Clean &    Save to
Databases               Process    destination
```

### Real-World Example:
1. **Extract**: Get user data from your Flutter app's database
2. **Transform**: Clean the data, calculate statistics
3. **Load**: Save to analytics database for reporting

---

## üõ† ETL Pipeline Implementation

### 1. Simple Python ETL Script

**`etl_pipeline.py`**
```python
import pandas as pd
import sqlite3
from datetime import datetime
import requests

class FlutterAppETL:
    def __init__(self):
        self.source_db = "app_data.db"
        self.target_db = "analytics.db"
    
    def extract_user_data(self):
        """Extract user data from app database"""
        print("üì• Extracting user data...")
        
        # Connect to source database
        conn = sqlite3.connect(self.source_db)
        
        # Extract user data
        users_query = """
        SELECT 
            user_id,
            username,
            email,
            created_at,
            last_login,
            total_sessions
        FROM users 
        WHERE created_at >= date('now', '-30 days')
        """
        
        users_df = pd.read_sql_query(users_query, conn)
        conn.close()
        
        return users_df
    
    def extract_app_usage(self):
        """Extract app usage data"""
        print("üì• Extracting app usage data...")
        
        conn = sqlite3.connect(self.source_db)
        
        usage_query = """
        SELECT 
            session_id,
            user_id,
            screen_name,
            time_spent,
            actions_count,
            session_date
        FROM app_sessions
        WHERE session_date >= date('now', '-30 days')
        """
        
        usage_df = pd.read_sql_query(usage_query, conn)
        conn.close()
        
        return usage_df
    
    def transform_data(self, users_df, usage_df):
        """Transform and clean the data"""
        print("üîÑ Transforming data...")
        
        # Clean user data
        users_df['email'] = users_df['email'].str.lower()
        users_df['created_at'] = pd.to_datetime(users_df['created_at'])
        users_df['last_login'] = pd.to_datetime(users_df['last_login'])
        
        # Calculate user metrics
        users_df['days_since_signup'] = (
            datetime.now() - users_df['created_at']
        ).dt.days
        
        users_df['is_active'] = users_df['last_login'] >= pd.Timestamp.now() - pd.Timedelta(days=7)
        
        # Aggregate usage data
        usage_summary = usage_df.groupby('user_id').agg({
            'time_spent': 'sum',
            'actions_count': 'sum',
            'session_id': 'count'
        }).rename(columns={'session_id': 'total_sessions'})
        
        # Join user data with usage summary
        final_df = users_df.merge(usage_summary, on='user_id', how='left')
        final_df = final_df.fillna(0)
        
        # Calculate engagement score
        final_df['engagement_score'] = (
            final_df['time_spent'] * 0.3 + 
            final_df['actions_count'] * 0.7
        )
        
        return final_df
    
    def load_data(self, transformed_df):
        """Load data to analytics database"""
        print("üì§ Loading data to analytics database...")
        
        # Connect to target database
        conn = sqlite3.connect(self.target_db)
        
        # Create table if not exists
        create_table_query = """
        CREATE TABLE IF NOT EXISTS user_analytics (
            user_id INTEGER,
            username TEXT,
            email TEXT,
            created_at TEXT,
            days_since_signup INTEGER,
            is_active BOOLEAN,
            total_sessions INTEGER,
            time_spent REAL,
            actions_count INTEGER,
            engagement_score REAL,
            processed_at TEXT
        )
        """
        
        conn.execute(create_table_query)
        
        # Add timestamp
        transformed_df['processed_at'] = datetime.now()
        
        # Load data
        transformed_df.to_sql('user_analytics', conn, if_exists='replace', index=False)
        
        conn.close()
        print(f"‚úÖ Loaded {len(transformed_df)} records")
    
    def run_etl(self):
        """Run the complete ETL process"""
        try:
            print("üöÄ Starting ETL Pipeline...")
            
            # Extract
            users_data = self.extract_user_data()
            usage_data = self.extract_app_usage()
            
            # Transform
            transformed_data = self.transform_data(users_data, usage_data)
            
            # Load
            self.load_data(transformed_data)
            
            print("üéâ ETL Pipeline completed successfully!")
            
        except Exception as e:
            print(f"‚ùå ETL Pipeline failed: {str(e)}")

# Run the ETL pipeline
if __name__ == "__main__":
    etl = FlutterAppETL()
    etl.run_etl()
```

### 2. Scheduled ETL with Cron

**`etl_scheduler.sh`**
```bash
#!/bin/bash

# Daily ETL pipeline runner
echo "üïí $(date): Starting daily ETL pipeline"

# Activate virtual environment
source /path/to/venv/bin/activate

# Run ETL script
python3 /path/to/etl_pipeline.py

# Check if successful
if [ $? -eq 0 ]; then
    echo "‚úÖ $(date): ETL pipeline completed successfully"
    
    # Send success notification (optional)
    curl -X POST -H 'Content-type: application/json' \
         --data '{"text":"ETL Pipeline completed successfully"}' \
         YOUR_SLACK_WEBHOOK_URL
else
    echo "‚ùå $(date): ETL pipeline failed"
    
    # Send failure notification
    curl -X POST -H 'Content-type: application/json' \
         --data '{"text":"ETL Pipeline FAILED - Please check logs"}' \
         YOUR_SLACK_WEBHOOK_URL
fi
```

**Crontab entry** (runs daily at 2 AM):
```bash
0 2 * * * /path/to/etl_scheduler.sh >> /var/log/etl.log 2>&1
```

### 3. Real-time ETL with Apache Airflow

**`flutter_app_dag.py`**
```python
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from datetime import datetime, timedelta

# Default arguments
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

# Create DAG
dag = DAG(
    'flutter_app_etl',
    default_args=default_args,
    description='Flutter App Data ETL Pipeline',
    schedule_interval='@daily',  # Run daily
    catchup=False
)

def extract_user_data():
    """Extract user data task"""
    # Your extraction logic here
    print("Extracting user data...")
    return "user_data_extracted"

def extract_usage_data():
    """Extract usage data task"""
    # Your extraction logic here
    print("Extracting usage data...")
    return "usage_data_extracted"

def transform_data():
    """Transform data task"""
    # Your transformation logic here
    print("Transforming data...")
    return "data_transformed"

def load_to_warehouse():
    """Load data to warehouse task"""
    # Your loading logic here
    print("Loading data to warehouse...")
    return "data_loaded"

# Define tasks
extract_users_task = PythonOperator(
    task_id='extract_user_data',
    python_callable=extract_user_data,
    dag=dag
)

extract_usage_task = PythonOperator(
    task_id='extract_usage_data',
    python_callable=extract_usage_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag
)

load_task = PythonOperator(
    task_id='load_to_warehouse',
    python_callable=load_to_warehouse,
    dag=dag
)

# Set task dependencies
[extract_users_task, extract_usage_task] >> transform_task >> load_task
```

---

## üì± Flutter App Data Collection for ETL

### 1. Analytics Service in Flutter

**`lib/services/analytics_service.dart`**
```dart
import 'package:sqflite/sqflite.dart';
import 'package:path/path.dart';

class AnalyticsService {
  static Database? _database;
  
  Future<Database> get database async {
    if (_database != null) return _database!;
    _database = await _initDB();
    return _database!;
  }
  
  Future<Database> _initDB() async {
    String path = join(await getDatabasesPath(), 'analytics.db');
    
    return await openDatabase(
      path,
      version: 1,
      onCreate: (db, version) async {
        // User sessions table
        await db.execute('''
          CREATE TABLE app_sessions (
            session_id TEXT PRIMARY KEY,
            user_id TEXT,
            screen_name TEXT,
            time_spent INTEGER,
            actions_count INTEGER,
            session_date TEXT
          )
        ''');
        
        // User events table
        await db.execute('''
          CREATE TABLE user_events (
            event_id TEXT PRIMARY KEY,
            user_id TEXT,
            event_type TEXT,
            event_data TEXT,
            timestamp TEXT
          )
        ''');
      },
    );
  }
  
  // Track screen time
  Future<void> trackScreenTime(String screenName, int timeSpent) async {
    final db = await database;
    
    await db.insert('app_sessions', {
      'session_id': DateTime.now().millisecondsSinceEpoch.toString(),
      'user_id': 'current_user_id',
      'screen_name': screenName,
      'time_spent': timeSpent,
      'actions_count': 1,
      'session_date': DateTime.now().toIso8601String(),
    });
  }
  
  // Track user events
  Future<void> trackEvent(String eventType, Map<String, dynamic> data) async {
    final db = await database;
    
    await db.insert('user_events', {
      'event_id': DateTime.now().millisecondsSinceEpoch.toString(),
      'user_id': 'current_user_id',
      'event_type': eventType,
      'event_data': jsonEncode(data),
      'timestamp': DateTime.now().toIso8601String(),
    });
  }
}
```

### 2. Using Analytics in Flutter Widgets

**`lib/screens/home_screen.dart`**
```dart
class HomeScreen extends StatefulWidget {
  @override
  _HomeScreenState createState() => _HomeScreenState();
}

class _HomeScreenState extends State<HomeScreen> {
  final AnalyticsService _analytics = AnalyticsService();
  late DateTime _screenStartTime;
  
  @override
  void initState() {
    super.initState();
    _screenStartTime = DateTime.now();
    
    // Track screen view
    _analytics.trackEvent('screen_view', {
      'screen_name': 'home',
      'timestamp': DateTime.now().toIso8601String(),
    });
  }
  
  @override
  void dispose() {
    // Track screen time when leaving
    final timeSpent = DateTime.now().difference(_screenStartTime).inSeconds;
    _analytics.trackScreenTime('home', timeSpent);
    super.dispose();
  }
  
  void _onButtonPressed(String buttonName) {
    // Track button press
    _analytics.trackEvent('button_press', {
      'button_name': buttonName,
      'screen': 'home',
    });
  }
  
  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(title: Text('Home')),
      body: Column(
        children: [
          ElevatedButton(
            onPressed: () => _onButtonPressed('feature_1'),
            child: Text('Feature 1'),
          ),
          ElevatedButton(
            onPressed: () => _onButtonPressed('feature_2'),
            child: Text('Feature 2'),
          ),
        ],
      ),
    );
  }
}
```

---

## üîß Complete CI/CD + ETL Workflow

### Integrated Pipeline Flow:

```
1. Developer pushes code
       ‚Üì
2. CI/CD runs tests & deploys app
       ‚Üì
3. App collects user data
       ‚Üì
4. ETL pipeline processes data
       ‚Üì
5. Analytics dashboard shows insights
       ‚Üì
6. Data-driven decisions for next features
```

### Docker-based ETL Pipeline

**`Dockerfile`**
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy ETL scripts
COPY etl_pipeline.py .
COPY config/ ./config/

# Run ETL pipeline
CMD ["python", "etl_pipeline.py"]
```

**`docker-compose.yml`**
```yaml
version: '3.8'

services:
  etl-pipeline:
    build: .
    environment:
      - DB_HOST=database
      - DB_USER=etl_user
      - DB_PASSWORD=secure_password
    depends_on:
      - database
    volumes:
      - ./data:/app/data
      - ./logs:/app/logs

  database:
    image: postgres:13
    environment:
      POSTGRES_DB: analytics
      POSTGRES_USER: etl_user
      POSTGRES_PASSWORD: secure_password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:
```

---

## üìä Monitoring & Alerts

### 1. Pipeline Monitoring Script

**`monitor_pipeline.py`**
```python
import time
import requests
from datetime import datetime

def check_pipeline_health():
    """Check if ETL pipeline is running correctly"""
    
    try:
        # Check database connectivity
        # Check recent data updates
        # Check error logs
        
        # Example health check
        last_update = get_last_etl_run()
        current_time = datetime.now()
        
        # Alert if ETL hasn't run in 25 hours (should run daily)
        if (current_time - last_update).hours > 25:
            send_alert("ETL Pipeline hasn't run in 25+ hours!")
            return False
        
        return True
        
    except Exception as e:
        send_alert(f"Pipeline health check failed: {str(e)}")
        return False

def send_alert(message):
    """Send alert to Slack/Email"""
    # Slack webhook example
    webhook_url = "YOUR_SLACK_WEBHOOK"
    payload = {"text": f"üö® ALERT: {message}"}
    requests.post(webhook_url, json=payload)

# Run health check
if __name__ == "__main__":
    if check_pipeline_health():
        print("‚úÖ Pipeline is healthy")
    else:
        print("‚ùå Pipeline issues detected")
```

---

## üéØ Best Practices

### CI/CD Best Practices:
1. **Test everything** - Unit, Widget, Integration tests
2. **Small, frequent deployments** - Easier to debug
3. **Rollback strategy** - Always have a way back
4. **Environment separation** - Dev, Staging, Production
5. **Security** - Keep secrets safe, scan for vulnerabilities

### ETL Best Practices:
1. **Data validation** - Check data quality at each step
2. **Error handling** - Graceful failure and recovery
3. **Monitoring** - Track pipeline performance
4. **Documentation** - Document data sources and transformations
5. **Scalability** - Design for growing data volumes

---

## üöÄ Tools & Services

### CI/CD Tools:
- **GitHub Actions** (Free, integrated with GitHub)
- **GitLab CI/CD** (Built into GitLab)
- **Jenkins** (Open source, self-hosted)
- **CircleCI** (Cloud-based)
- **Azure DevOps** (Microsoft ecosystem)

### ETL Tools:
- **Apache Airflow** (Python-based, very popular)
- **Apache NiFi** (Visual ETL designer)
- **AWS Glue** (Managed ETL service)
- **Google Cloud Dataflow** (Serverless ETL)
- **Custom Python scripts** (Simple, flexible)

---

## üéâ Summary

**CI/CD** automates testing and deployment, making your development process faster and more reliable.

**ETL pipelines** help you process and analyze data from your Flutter app to make better decisions.

Together, they create a complete automated workflow from code to insights! üöÄ